# -*- coding: utf-8 -*-
"""Urban Cemeteries_MP Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mco61VeRgjgcW2g2cltrOpD7wsSDlqrl

Subject : URPL-GP-1620
          Spatial Analysis and Data Visualization Fallâ€™ 19

Project: Pujara, Maitri
Team : Alsina, Celeste | Pujara, Maitri | Rosa, Megan


---



Project Part-1 : I am here to explore the demand for 
cemeteries and look at death rates and data on how many 
people are choosing cremation over burials.

1. Pre-installed common libraries 

---



The two main libraries used for data manipulation in python are numpy which provides the data type and functions for arrays, and pandas which builds on numpy and provides the the data type and functions for dataframes.
"""

import pandas as pd
import numpy as np

"""2. Import the dataset into the google collab



---


If you need to use a library that isn't already available in Colab you'll need to install it yourself. For example, someone has made a copy of R's janitor package for Python but we'll have to install it first. Colab has a nice feature that in the left side bar they have a bunch of helpful Code
"""

pd.read_csv('https://drive.google.com/uc?export=download&id=1ByQvRMFCcL6ZtynM7SxeczmtkpaaMVgN')

"""3. Import libraries called 'Janitor'


---


If you need to use a library that isn't already available in Colab you'll need to install it yourself. For example, someone has made a copy of R's janitor package for Python but we'll have to install it first. Colab has a nice feature that in the left side bar they have a bunch of helpful Code
"""

!pip install -q pyjanitor
import janitor

"""4. Import dataset and clean names of the dataset and Importing CSV files



---

Now we can import our dataset with the read_csv function from pandas, giving it the URL for our CSV file, and assigning the resulting dataframe to Vital_Statistics.raw. We can then overwrite the Vital_Statistics_raw with a version that has all the column names reformatted by using the clean_names function from janitor.

"""

Vital_Statistics_raw = pd.read_csv('https://drive.google.com/uc?export=download&id=1ByQvRMFCcL6ZtynM7SxeczmtkpaaMVgN')
Vital_Statistics_raw = janitor.clean_names(Vital_Statistics_raw)

"""5. Previewing dataframes



---

Now we have a dataframe of our pluto data called Vital_Statistics_raw. In python objects that you create like this dataframe have functions associated with them called "methods" and you can use these like dataframe_name.method_name()


"""

Vital_Statistics_raw.head()

"""6. In addition to step-5



---


Then we can also use the info method to see all the column names, the number of non-missing rows, and the data type. This is similar to dplyr's glimpse function in R. 

"""

Vital_Statistics_raw.info()

"""7. Selecting/dropping/renaming columns



---

We can select a subset of the columns using a few different methods. These are equivilant to dplyr's select function in R.
"""

Vital_Statistics_slim = Vital_Statistics_raw.loc[:,['year','county_name','region','place_of_death_code','place_of_death_description','disposition_code','disposition_description','deaths']]
Vital_Statistics_slim = Vital_Statistics_slim.rename(columns={'Disposition Description':'dd'})
Vital_Statistics_slim = Vital_Statistics_slim.drop(['place_of_death_code', 'place_of_death_description', 'disposition_code'], axis=1)
Vital_Statistics_slim.head()

"""8. Sorting dataframes



---

Now we can sort the rows in our dataframe to see the 5 buildings with the most total units. This is similar to dplyr's arrange function in R. In this case we just want to see it, but don't need to make a permant change to the dataframe so we won't assign the result to anything but just print the head at the end.
"""

Vital_Statistics_slim.sort_values('deaths', ascending=False).head()
vital_new = Vital_Statistics_slim.clean_names()
vital_new

"""9. Method Chaining



---


As you can see above, when we have a dataframe and use a method that returns a dataframe we and then use another method on that result. This is called "method chaining" because we can chain together multiple calls to methods on an object. This achieves a similar goal as the %>% (pipe) operator in R's tidyverse packages. As you can imagine your code can get a little hard to read when you do a lot of this chaining all on one line. However, unlike R, in Python indentation has special meaning so we have to be careful with formating our code. To group together section where we are using method chaining we need to wrap the whole thing in parentheses.
"""

queens_nyc = (
    vital_new
      .query("county_name == 'Queens'")
      .query("region == 'NYC'")
      .query("disposition_description == 'Cremation' or disposition_description == 'Burial' ")
      .groupby(['disposition_description','year','region','county_name'])
      .sum()
      .reset_index()
      .sort_values('deaths', ascending = False)
      )

queens_nyc

"""10. Filtering rows with query



---


We can use the query method on our dataframe to filter to only the buildings in Queens. This is similar to the dplyr's filter function in R.
"""

queens_nyc = (
    vital_new
      .query("county_name == 'Queens'")
      .query("region == 'NYC'")
      .query("disposition_description == 'Cremation' or disposition_description == 'Burial' ")
      .groupby(['disposition_description','year'])
      .sum()
      .reset_index()
      .sort_values('deaths', ascending = False)
      )

queens_nyc

"""11. Creating new columns



---


To create new columns in our dataframe we can use the method assign. This works similarly to dplyr's mutate function in R. Note that, unlike mutate, we need to specify the columns with either dataframe_name.column_name or dataframe_name['column_name']. Similar to in R, to use the dataframe_name.column_name style your column names can't have spaces or special characters. Luckily we've already used janitor.clean_names() so we won't have any problems.
"""

queens_nyc.sort_values(by=['year','disposition_description'], ascending=False)

"""12. Format output with style


---

After using the dataset, one can count the total number of deaths in Queens county, NYC
"""

('{:,}').format(sum_deaths)

deaths_by_region

"""13. Grouping and aggregating



---

We can aggregate our dataframe by values of an existing column using the groupby method on our dataframe in combindation with one of a few different aggregation methods. This is similar to the pattern of dplyr's group_by and summarize in R.

Like with dplyr's group_by() and ungroup(), it's important to "ungroup" your datagrame after your aggregation using reset_index()
"""

queens_nyc.groupby(['disposition_description']).count().reset_index().head()

queens_nyc.groupby(['deaths']).mean().reset_index().head()

queens_nyc.groupby(['disposition_description']).agg({'deaths':'sum'}).reset_index().head()

"""14. Plotting with matplotlib


---


Matplotlib is the most common plotting package and one can add other additional packages as well.
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
#!pip install -q pyjanitor
import janitor

"""15. Import and Clean Dataset

---


"""

queens_nyc = queens_nyc.pivot(index = 'year', columns = 'disposition_description').reset_index()
#it is important to reset the index here
queens_nyc.head()

"""16. Import and Clean Dataset


---


"""

queens_nyc = (
    vital_new
      .query("county_name == 'Queens'")
      .query("region == 'NYC'")
      .query("disposition_description == 'Cremation' or disposition_description == 'Burial' ")
      .groupby(['disposition_description','year'])
      .sum()
      .reset_index()
      .sort_values('deaths', ascending = False)
      )

queens_nyc

queens_pivoted = queens_nyc.pivot(index ='year', columns ='disposition_description', values ='deaths').reset_index()
queens_pivoted.head()

"""17. Creating a Bar Chart-1"""

x1 = ['20O3','2004','2005','2006','2007']
y1 = [11543, 10979, 10942, 10560, 10227]
y2 = [3006, 3002, 3060, 3104, 3197]

# Colors: https://matplotlib.org/api/colors_api.html

plt.bar(x1, y1, label="Burial", color='b')
plt.bar(x1, y2, label="Cremation", color='g')
plt.plot()

plt.xlabel("Year")
plt.ylabel("disposition_description")
plt.title("Burial Vs Cremation in Queens County, NYC")
plt.legend()
plt.show()

"""Source: Vital Statistics Deaths by Resident County, Region, Place of Death: Beginning 2003, NYC Gov

18. Creating a Graph-2


---
"""

y1 = queens_pivoted['Burial']
y2 = queens_pivoted['Cremation']
years = ['2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']
X = np.arange(15)
plt.xticks(X,years) 

plt.bar(X + 0.00, y1, label="Burial", color = '#1f78b4', width = 0.40)
plt.bar(X + 0.40, y2, label="Cremation", color = '#beaed4', width = 0.40)

plt.xticks(rotation=-45, horizontalalignment="left")

plt.plot()

plt.xlabel("Year")
plt.ylabel("Deaths")
plt.title("Burial Vs Cremation in Queens County, NYC")
plt.legend()
plt.show()

plt.figure(figsize=(15,6))
plt.plot( 'year', 'Burial', data=queens_pivoted, color='blue', label= "Burial")
plt.plot( 'year', 'Cremation', data=queens_pivoted, color='purple', label= "Cremation")
#2001 is the starting point, 2017 is the end point, and 1 is the interval
plt.xlim(2003, 2030)
plt.xticks(np.arange(2003, 2030, 1))
plt.xlabel("Year")
plt.ylabel("Deaths")
plt.title("Burial Vs Cremation in Queens County, NYC 2003-2017")
plt.legend()
plt.show()



"""Source: Vital Statistics Deaths by Resident County, Region, Place of Death: Beginning 2003, NYC Gov

19. Creating a Graph-3


---
"""

x1 = ['2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']
y1 = [11543, 10979, 10942, 10560, 10227, 9740, 9392, 9249, 9095, 9407, 8957, 8914, 8572, 8710, 8568]
y2 = [3006, 3002, 3060, 3104, 3197, 3377, 3586, 3722, 3869, 3911, 4119, 4383, 4521, 4931, 5163]

# Colors: https://matplotlib.org/api/colors_api.html

plt.bar(x1, y1, label="Burial", color='b')
plt.bar(x1, y2, label="Cremation", color='g')
plt.plot()

plt.xlabel("Year")
plt.ylabel("disposition_description")
plt.title("Burial Vs Cremation in Queens County, NYC")
plt.legend()
plt.show()

"""Source : Vital Statistics Deaths by Resident County, Region, Place of Death: Beginning 2003, NYC Gov

20. Creating a Graph-4


---
"""

# Use numpy to generate a bunch of random data in a bell curve around 5.
n = 5 + np.random.randn(15)

m = [m for m in range(len(n))]
plt.bar(m, n)
plt.title("Burial Vs Cremation in Queens County, NYC")
plt.show()

plt.hist(n, bins=20)
plt.title("Histogram")
plt.show()

plt.hist(n, cumulative=True, bins=20)
plt.title("Cumulative Histogram")
plt.show()

"""21. Creating a Graph-5


---


"""

import altair as alt

alt.Chart(queens_pivoted).mark_bar(color='goldenrod').encode(
    x=alt.X(
        'Burial',
        axis=alt.Axis( format='%', title='deaths')
    ),
    y=alt.Y(
        'Cremation',
        sort=alt.EncodingSortField(
            field="disposition_description"
        ),
        axis=alt.Axis(title='Burial Vs Cremation in Queens County, NYC')
        )
    )

"""22. Creating a Graph-6


---


"""

bars = alt.Chart(queens_pivoted).mark_bar(color='goldenrod').encode(
  x=alt.X(
      'Burial',
      axis=alt.Axis(format='%', title='deaths')
  ),
  y=alt.Y(
    'Cremation',
    sort=alt.EncodingSortField(
      field="deaths",    # The field to use for the sort
      order="descending" # The order to sort in
    ),
    axis=alt.Axis(title='Burial Vs Cremation in Queens County, NYC')
  )
)

text = bars.mark_text(
    align='left',
    baseline='middle',
    dx=3  # Nudges text to right so it doesn't appear on top of the bar
).encode(
    text=alt.Text('Cremation', format=".1%")
)

(bars + text).properties(
    title='Share of "Housing New York" Units that are New Construction, Queens Community Districts',
    # width=800,
    # height=600
)

"""Project Part-2 : I am here to explore all the counties in NewYork City considering all death rates.


---

1. Pre-installed common libraries 

---



The two main libraries used for data manipulation in python are numpy which provides the data type and functions for arrays, and pandas which builds on numpy and provides the the data type and functions for dataframes.



---
2. Import the dataset into the google collab



---


If you need to use a library that isn't already available in Colab you'll need to install it yourself. For example, someone has made a copy of R's janitor package for Python but we'll have to install it first. Colab has a nice feature that in the left side bar they have a bunch of helpful Code
"""

import pandas as pd
import numpy as np


pd.read_csv('https://drive.google.com/uc?export=download&id=1ByQvRMFCcL6ZtynM7SxeczmtkpaaMVgN')

"""3. Import libraries called 'Janitor'


---


If you need to use a library that isn't already available in Colab you'll need to install it yourself. For example, someone has made a copy of R's janitor package for Python but we'll have to install it first. Colab has a nice feature that in the left side bar they have a bunch of helpful Code
"""

!pip install -q pyjanitor
import janitor
Vital_Statistics_raw = pd.read_csv('https://drive.google.com/uc?export=download&id=1ByQvRMFCcL6ZtynM7SxeczmtkpaaMVgN')
Vital_Statistics_raw = janitor.clean_names(Vital_Statistics_raw)
Vital_Statistics_raw.head()

"""4. Import dataset and clean names of the dataset and Importing CSV files



---

Now we can import our dataset with the read_csv function from pandas, giving it the URL for our CSV file, and assigning the resulting dataframe to Vital_Statistics.raw. We can then overwrite the Vital_Statistics_raw with a version that has all the column names reformatted by using the clean_names function from janitor.



---
5. Previewing dataframes



---

Now we have a dataframe of our pluto data called Vital_Statistics_raw. In python objects that you create like this dataframe have functions associated with them called "methods" and you can use these like dataframe_name.method_name()

"""

Vital_Statistics_raw.info()

"""6. Selecting/dropping/renaming columns



---

We can select a subset of the columns using a few different methods. These are equivilant to dplyr's select function in R.
"""

Vital_Statistics_slim = Vital_Statistics_raw.loc[:,['year', 'county_name', 'region', 'place_of_death_code', 'place_of_death_description', 'disposition_code', 'disposition_description', 'deaths']]
Vital_Statistics_slim = Vital_Statistics_slim.drop(['place_of_death_code', 'disposition_code', 'place_of_death_description'], axis=1)
Vital_Statistics_slim = Vital_Statistics_slim.rename(columns={'disposition_description':'B-C'})
Vital_Statistics_slim = Vital_Statistics_slim.rename(columns={'county_name':'county'})
Vital_Statistics_slim.head()

"""7. Sorting dataframes



---

Now we can sort the rows in our dataframe to see the 5 buildings with the most total units. This is similar to dplyr's arrange function in R. In this case we just want to see it, but don't need to make a permant change to the dataframe so we won't assign the result to anything but just print the head at the end.
"""

NYC_bldgs = Vital_Statistics_raw.query("region == 'NYC'")

NYC_bldgs.head()

"""8. Selecting/dropping/renaming columns



---

We can select a subset of the columns using a few different methods. These are equivilant to dplyr's select function in R.
"""

Vital_Statistics_slim = Vital_Statistics_raw.loc[:,['year', 'county_name', 'region', 'place_of_death_code', 'place_of_death_description', 'disposition_code', 'disposition_description', 'deaths']]
Vital_Statistics_slim = Vital_Statistics_raw.query("region == 'NYC'")
Vital_Statistics_slim = Vital_Statistics_slim.drop(['place_of_death_code', 'disposition_code', 'place_of_death_description'], axis=1)
Vital_Statistics_slim = Vital_Statistics_slim.rename(columns={'disposition_description':'B-C'})
Vital_Statistics_slim = Vital_Statistics_slim.rename(columns={'county_name':'county'})
Vital_Statistics_slim.head()

"""9. Sorting dataframes



---

Now we can sort the rows in our dataframe to see the 5 buildings with the most total units. This is similar to dplyr's arrange function in R. In this case we just want to see it, but don't need to make a permant change to the dataframe so we won't assign the result to anything but just print the head at the end.
"""

Vital_Statistics_slim.sort_values('deaths', ascending=False).head()
vital_new = Vital_Statistics_slim.clean_names()
vital_new

"""10. Filtering rows with query



---


We can use the query method on our dataframe to filter to only the buildings in Queens. This is similar to the dplyr's filter function in R.
"""

queens_nyc = (
    vital_new
      .query("county == 'Queens' or  county == 'Bronx' or county == 'Kings' or county == 'Manhattan' or county == 'Richmond' ")
      .query("region == 'NYC'")
      .query("b_c == 'Cremation' or b_c == 'Burial' ")
      .groupby(['b_c','year','deaths','county'])
      .sum()
      .reset_index()
      .sort_values('deaths', ascending=False)
)

queens_nyc.head()

queens_nyc_slim = (
    queens_nyc
      .query("county == 'Queens' or  county == 'Bronx' or county == 'Kings' or county == 'Manhattan' or county == 'Richmond' ")
      .query("b_c == 'Cremation' or b_c == 'Burial' ")
      .groupby(['b_c','year','deaths','county'])
      .sum()
      .reset_index()
      .sort_values('deaths', ascending=False)
    )
queens_nyc_slim.head()

"""11. Creating new columns



---


To create new columns in our dataframe we can use the method assign. This works similarly to dplyr's mutate function in R. Note that, unlike mutate, we need to specify the columns with either dataframe_name.column_name or dataframe_name['column_name']. Similar to in R, to use the dataframe_name.column_name style your column names can't have spaces or special characters. Luckily we've already used janitor.clean_names() so we won't have any problems.
"""

queens_cd_units = (queens_nyc_slim
  .groupby(['county'])
  .agg(total_deaths_avg=('deaths', 'mean'))
  .reset_index()
  .head()
)

queens_cd_units

"""12. Creating new column of 'Total_Deaths_Average'

---


"""

queens_cd_units.style.format({
  'total_units_avg': '{:.1f}'.format,
})

"""13. Creating a Graph-2


---


"""

import altair as alt
alt.Chart(queens_cd_units).mark_bar(color='goldenrod').encode(
  x=alt.X(
      'total_deaths_avg',
      axis=alt.Axis(title='Average Total Death per County')
  ),
  y=alt.Y(
    'county',
    sort=alt.EncodingSortField(
      field="total_deaths_avg",    # The field to use for the sort
      order="descending" # The order to sort in
    ),
    axis=alt.Axis(title='County in NYC')
  )
)

"""14. Creating a Graph-2 (Extra)

---

Unlike R where ggplot2 is pretty much the univsal standard for graphing, in Python there are a lot of different packages that are commonly used. matplotlib is one very popular one, seaborn is a libray based on matplotlib that offers a simpler syntax and nicer defaults.

Google Colab notebooks have a helpful sidepane with code snippets that you can browse and automatically add into your notebook. These snippets all use a different newer plotting library called altair. Here we build from some of the snippets provided to visualize our housing dataset.

Here is the snippet eactly as Colab provides it. Note that there are lots of parts here that you can cut out. To make the example work on it's own they import some sampe data, but we have our own data to use so we can remove that and swap in our dataset. We'll start by using the dataset we already summarized for stats about Queens commmunity districts.
"""

alt.Chart(queens_cd_units).mark_bar().encode(
  x='total_deaths_avg',
  y='county',
)